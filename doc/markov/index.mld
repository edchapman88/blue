{0 The [markov] library}

[markov] defines a {e reusable} abstract software interface for {b online learning agents} that interact with {b real systems}.

Many existing applications of online learning agents to problems, both inside and outside of Autonomous Cyber Defence (ACD), involve two significant constraints:
+ The agent interacts with a {b discrete-time} system, representable by a {e state machine}.
+ The interactions between the agent and it's environment are {b turn-based}.

{b Real-world} systems are typically {b continuous-time} and {b concurrently-running}. [markov] presents a simple model that generalises over the two constraints above, to provide a {e reusable} software interface for the application of online learning agents to continuous-time, concurrently-running systems. 

{1 Design}
The digraph below shows how an agent's policy consumes a {e state}, returning:
+ A {b new policy}. In the case where this policy is different from the previous policy, the agent is {e 'training'}.
+ An {b action} to be executed in the surrounding system.
+ An {b 'observer'}: a function that will return a new {e state}.

Requiring that the policy return an {e observer} introduces a helpful {b generalisation to continuous-time systems} - one in which the policy itself controls {e when} observations occur, and feasibly {e what} they consist of.

{image!policy-steps.png}

E.g., in the context of cyber defence, consider two scenarios:
+ In response to a {b high threat state} a policy might produce an {e observer} function that returns {b quickly} with information that is critical to that {b specific} threat.
+ Alternatively, in a {b low threat state}, produce an observer function returning after a {b longer delay}, perhaps with a state containing a {b broader} set of information.

In the application of online learning to {e real} systems, {b resource bounds will influence the optimal behaviour of the agent}. The {e observer} model allows the policy itself to control and optimise over the challenges posed by resource constraints.

{1 The {{!Markov.Agent}[Agent]} module}
The [Agent] module exposes a functor (what is a {{:https://ocaml.org/docs/functors}functor}?), {{!Markov.Agent.Make}[Agent.Make]}, that when called will return a module of type {{!Markov.Agent.S}[Agent.S]}.

Example usage:
{[
module MarkovCompressor = struct
(* Your implementation here *) ...
end

module Reward = struct
(* Your implementation here *) ...
end

module Policy = struct
(* Your implementation here *) ...
end

module Agent = Markov.Agent.Make (MarkovCompressor) (Reward) (Policy)

let () =
  Agent.act (Agent.init_policy ())
]}

{2 The {{!Markov.Agent.S.act}[Agent.S.act]} loop}
In the example above,
{[
  Agent.act (Agent.init_policy ())
]}
kicks off an infinite loop modelling an Markov Decision Process (MDP). Each iteration of the loop involves:
+ measuring the state of the MDP
+ infering an action to take
+ infering a method to next measure the state of the MDP (in other words producing an {e observer})
+ executing the action

{b The implementation of this loop is included in the [markov] library.} It is found in {{!/markov/src/agent.ml.html#module-Make}the implementation} of the [Agent.Make] functor.
